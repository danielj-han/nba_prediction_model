{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from datetime import timedelta\n",
    "from sklearn.model_selection import TimeSeriesSplit, GridSearchCV\n",
    "from sklearn.metrics import accuracy_score\n",
    "from xgboost import XGBClassifier\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.pipeline import Pipeline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# --------------------------------------------------------------------\n",
    "# 1. LOAD DATA\n",
    "# --------------------------------------------------------------------\n",
    "df = pd.read_csv('data/games.csv', parse_dates=['GAME_DATE_EST'])\n",
    "\n",
    "# We will use 'x' as the number of recent games for rolling averages.\n",
    "x = 35"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# --------------------------------------------------------------------\n",
    "# 2. CONVERT DATA TO LONG FORMAT\n",
    "#    One row = (TEAM_ID, date, PTS, FG_PCT, FT_PCT, FG3_PCT, AST, REB, home/away)\n",
    "# --------------------------------------------------------------------\n",
    "home_stats = df[['GAME_ID', 'GAME_DATE_EST', 'HOME_TEAM_ID', \n",
    "                 'PTS_home', 'FG_PCT_home', 'FT_PCT_home', 'FG3_PCT_home',\n",
    "                 'AST_home', 'REB_home']].copy()\n",
    "home_stats.rename(columns={\n",
    "    'HOME_TEAM_ID': 'TEAM_ID',\n",
    "    'PTS_home': 'PTS',\n",
    "    'FG_PCT_home': 'FG_PCT',\n",
    "    'FT_PCT_home': 'FT_PCT',\n",
    "    'FG3_PCT_home': 'FG3_PCT',\n",
    "    'AST_home': 'AST',\n",
    "    'REB_home': 'REB'\n",
    "}, inplace=True)\n",
    "home_stats['home_away'] = 'home'\n",
    "\n",
    "away_stats = df[['GAME_ID', 'GAME_DATE_EST', 'TEAM_ID_away',\n",
    "                 'PTS_away', 'FG_PCT_away', 'FT_PCT_away', 'FG3_PCT_away',\n",
    "                 'AST_away', 'REB_away']].copy()\n",
    "away_stats.rename(columns={\n",
    "    'TEAM_ID_away': 'TEAM_ID',\n",
    "    'PTS_away': 'PTS',\n",
    "    'FG_PCT_away': 'FG_PCT',\n",
    "    'FT_PCT_away': 'FT_PCT',\n",
    "    'FG3_PCT_away': 'FG3_PCT',\n",
    "    'AST_away': 'AST',\n",
    "    'REB_away': 'REB'\n",
    "}, inplace=True)\n",
    "away_stats['home_away'] = 'away'\n",
    "\n",
    "team_stats_long = pd.concat([home_stats, away_stats], ignore_index=True)\n",
    "team_stats_long.sort_values(['TEAM_ID', 'GAME_DATE_EST'], inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/var/folders/vf/p2b2f4r533b70rfyzb4cm9cr0000gn/T/ipykernel_23542/3449259733.py:25: DeprecationWarning: DataFrameGroupBy.apply operated on the grouping columns. This behavior is deprecated, and in a future version of pandas the grouping columns will be excluded from the operation. Either pass `include_groups=False` to exclude the groupings or explicitly select the grouping columns after groupby to silence this warning.\n",
      "  team_stats_long = team_stats_long.groupby('TEAM_ID').apply(compute_rolling_and_rest)\n"
     ]
    }
   ],
   "source": [
    "# --------------------------------------------------------------------\n",
    "# 3. COMPUTE ROLLING AVERAGES & REST DAYS\n",
    "#    - rolling averages for last x games (e.g., PTS, FG%, etc.)\n",
    "#    - rest days: difference (in days) between consecutive games for the same team\n",
    "# --------------------------------------------------------------------\n",
    "def compute_rolling_and_rest(group):\n",
    "    group = group.sort_values('GAME_DATE_EST')\n",
    "    \n",
    "    # Calculate REST_DAYS as difference from previous game date\n",
    "    group['REST_DAYS'] = group['GAME_DATE_EST'].diff().dt.days\n",
    "    \n",
    "    # shift(1) so the current game doesn't include itself in rolling stats\n",
    "    group['rolling_PTS']     = group['PTS'].shift(1).rolling(window=x, min_periods=x).mean()\n",
    "    group['rolling_FG_PCT']  = group['FG_PCT'].shift(1).rolling(window=x, min_periods=x).mean()\n",
    "    group['rolling_FT_PCT']  = group['FT_PCT'].shift(1).rolling(window=x, min_periods=x).mean()\n",
    "    group['rolling_FG3_PCT'] = group['FG3_PCT'].shift(1).rolling(window=x, min_periods=x).mean()\n",
    "    group['rolling_AST']     = group['AST'].shift(1).rolling(window=x, min_periods=x).mean()\n",
    "    group['rolling_REB']     = group['REB'].shift(1).rolling(window=x, min_periods=x).mean()\n",
    "    \n",
    "    # shift(1) for REST_DAYS so that the rest for the current game is from the previous game\n",
    "    group['REST_DAYS'] = group['REST_DAYS'].shift(1)\n",
    "    \n",
    "    return group\n",
    "\n",
    "team_stats_long = team_stats_long.groupby('TEAM_ID').apply(compute_rolling_and_rest)\n",
    "team_stats_long = team_stats_long.dropna(subset=['rolling_PTS'])  # ensure at least x prior games\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# --------------------------------------------------------------------\n",
    "# 4. MERGE ROLLING & REST FEATURES BACK INTO WIDE FORMAT\n",
    "# --------------------------------------------------------------------\n",
    "def get_team_rolling_stats(game_date, team_id, team_stats):\n",
    "    # Filter to games for this team before 'game_date'\n",
    "    subset = team_stats[(team_stats['TEAM_ID'] == team_id) & (team_stats['GAME_DATE_EST'] < game_date)]\n",
    "    if subset.empty:\n",
    "        return pd.Series({\n",
    "            'rolling_PTS': np.nan,\n",
    "            'rolling_FG_PCT': np.nan,\n",
    "            'rolling_FT_PCT': np.nan,\n",
    "            'rolling_FG3_PCT': np.nan,\n",
    "            'rolling_AST': np.nan,\n",
    "            'rolling_REB': np.nan,\n",
    "            'REST_DAYS': np.nan\n",
    "        })\n",
    "    else:\n",
    "        # Return the last row (most recent game) for that team before 'game_date'\n",
    "        last_row = subset.iloc[-1]\n",
    "        return pd.Series({\n",
    "            'rolling_PTS': last_row['rolling_PTS'],\n",
    "            'rolling_FG_PCT': last_row['rolling_FG_PCT'],\n",
    "            'rolling_FT_PCT': last_row['rolling_FT_PCT'],\n",
    "            'rolling_FG3_PCT': last_row['rolling_FG3_PCT'],\n",
    "            'rolling_AST': last_row['rolling_AST'],\n",
    "            'rolling_REB': last_row['rolling_REB'],\n",
    "            'REST_DAYS': last_row['REST_DAYS']\n",
    "        })\n",
    "\n",
    "# Create columns for home & away rolling stats + rest days\n",
    "for stat in ['rolling_PTS', 'rolling_FG_PCT', 'rolling_FT_PCT', 'rolling_FG3_PCT', 'rolling_AST', 'rolling_REB', 'REST_DAYS']:\n",
    "    df[f'home_{stat}'] = np.nan\n",
    "    df[f'away_{stat}'] = np.nan\n",
    "\n",
    "for idx, row in df.iterrows():\n",
    "    home_info = get_team_rolling_stats(row['GAME_DATE_EST'], row['HOME_TEAM_ID'], team_stats_long)\n",
    "    away_info = get_team_rolling_stats(row['GAME_DATE_EST'], row['TEAM_ID_away'], team_stats_long)\n",
    "    \n",
    "    df.at[idx, 'home_rolling_PTS']     = home_info['rolling_PTS']\n",
    "    df.at[idx, 'home_rolling_FG_PCT']  = home_info['rolling_FG_PCT']\n",
    "    df.at[idx, 'home_rolling_FT_PCT']  = home_info['rolling_FT_PCT']\n",
    "    df.at[idx, 'home_rolling_FG3_PCT'] = home_info['rolling_FG3_PCT']\n",
    "    df.at[idx, 'home_rolling_AST']     = home_info['rolling_AST']\n",
    "    df.at[idx, 'home_rolling_REB']     = home_info['rolling_REB']\n",
    "    df.at[idx, 'home_REST_DAYS']       = home_info['REST_DAYS']\n",
    "    \n",
    "    df.at[idx, 'away_rolling_PTS']     = away_info['rolling_PTS']\n",
    "    df.at[idx, 'away_rolling_FG_PCT']  = away_info['rolling_FG_PCT']\n",
    "    df.at[idx, 'away_rolling_FT_PCT']  = away_info['rolling_FT_PCT']\n",
    "    df.at[idx, 'away_rolling_FG3_PCT'] = away_info['rolling_FG3_PCT']\n",
    "    df.at[idx, 'away_rolling_AST']     = away_info['rolling_AST']\n",
    "    df.at[idx, 'away_rolling_REB']     = away_info['rolling_REB']\n",
    "    df.at[idx, 'away_REST_DAYS']       = away_info['REST_DAYS']\n",
    "\n",
    "# Drop rows where we lack rolling stats or rest days for either team\n",
    "df = df.dropna(subset=[\n",
    "    'home_rolling_PTS', 'away_rolling_PTS',\n",
    "    'home_REST_DAYS', 'away_REST_DAYS'\n",
    "])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# --------------------------------------------------------------------\n",
    "# 5. CREATE DIFFERENCE FEATURES\n",
    "# --------------------------------------------------------------------\n",
    "# We'll create difference features for rolling stats and for REST_DAYS as well.\n",
    "for stat in ['rolling_PTS', 'rolling_FG_PCT', 'rolling_FT_PCT', 'rolling_FG3_PCT', 'rolling_AST', 'rolling_REB', 'REST_DAYS']:\n",
    "    df[f'diff_{stat}'] = df[f'home_{stat}'] - df[f'away_{stat}']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# --------------------------------------------------------------------\n",
    "# 6. SET UP FEATURE MATRIX (X) AND TARGET (y)\n",
    "# --------------------------------------------------------------------\n",
    "feature_cols = (\n",
    "    [f'home_{stat}' for stat in ['rolling_PTS', 'rolling_FG_PCT', 'rolling_FT_PCT', 'rolling_FG3_PCT', 'rolling_AST', 'rolling_REB', 'REST_DAYS']] +\n",
    "    [f'away_{stat}' for stat in ['rolling_PTS', 'rolling_FG_PCT', 'rolling_FT_PCT', 'rolling_FG3_PCT', 'rolling_AST', 'rolling_REB', 'REST_DAYS']] +\n",
    "    [f'diff_{stat}' for stat in ['rolling_PTS', 'rolling_FG_PCT', 'rolling_FT_PCT', 'rolling_FG3_PCT', 'rolling_AST', 'rolling_REB', 'REST_DAYS']]\n",
    ")\n",
    "\n",
    "X = df[feature_cols].copy()\n",
    "y = df['HOME_TEAM_WINS'].copy()  # 1 if home team wins, 0 otherwise"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# --------------------------------------------------------------------\n",
    "# 7. TIME SERIES SPLIT\n",
    "# --------------------------------------------------------------------\n",
    "# We sort by date so that earlier games come before later games.\n",
    "df_sorted = df.sort_values(by='GAME_DATE_EST')\n",
    "X_sorted = X.loc[df_sorted.index]\n",
    "y_sorted = y.loc[df_sorted.index]\n",
    "\n",
    "# We'll use a 5-fold time series split for demonstration.\n",
    "tscv = TimeSeriesSplit(n_splits=5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fitting 5 folds for each of 48 candidates, totalling 240 fits\n",
      "Best parameters: {'xgb__colsample_bytree': 0.8, 'xgb__learning_rate': 0.01, 'xgb__max_depth': 6, 'xgb__n_estimators': 300, 'xgb__subsample': 0.8}\n"
     ]
    }
   ],
   "source": [
    "# --------------------------------------------------------------------\n",
    "# 8. XGBOOST + GRID SEARCH\n",
    "# --------------------------------------------------------------------\n",
    "model = XGBClassifier(eval_metric='logloss', random_state=42)\n",
    "\n",
    "param_grid = {\n",
    "    'xgb__n_estimators': [100, 300],\n",
    "    'xgb__max_depth': [3, 6, 9],\n",
    "    'xgb__learning_rate': [0.01, 0.1],\n",
    "    'xgb__subsample': [0.8, 1.0],\n",
    "    'xgb__colsample_bytree': [0.8, 1.0]\n",
    "}\n",
    "\n",
    "\n",
    "# We'll do a pipeline with scaling + XGB. (XGBoost often handles unscaled data well,\n",
    "# but scaling won't hurt, especially if we consider alternative models later.)\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "\n",
    "pipeline = Pipeline([\n",
    "    ('scaler', StandardScaler()),\n",
    "    ('xgb', model)\n",
    "])\n",
    "\n",
    "grid_search = GridSearchCV(\n",
    "    pipeline, \n",
    "    param_grid, \n",
    "    scoring='accuracy', \n",
    "    cv=tscv, \n",
    "    verbose=1,\n",
    "    n_jobs=-1\n",
    ")\n",
    "\n",
    "grid_search.fit(X_sorted, y_sorted)\n",
    "print(\"Best parameters:\", grid_search.best_params_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Final Accuracy (on entire dataset): 0.6710698605869213\n"
     ]
    }
   ],
   "source": [
    "# --------------------------------------------------------------------\n",
    "# 9. FINAL EVALUATION\n",
    "# --------------------------------------------------------------------\n",
    "best_model = grid_search.best_estimator_\n",
    "y_pred = best_model.predict(X_sorted)\n",
    "acc = accuracy_score(y_sorted, y_pred)\n",
    "print(\"Final Accuracy (on entire dataset):\", acc)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pickle\n",
    "\n",
    "# Save the best estimator (the full pipeline)\n",
    "with open('xgb_model.pkl', 'wb') as f:\n",
    "    pickle.dump(best_model, f)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "myenv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
